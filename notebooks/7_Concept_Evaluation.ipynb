{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Concept Evaluation\n",
    "\n",
    "This notebook demonstrates how to evaluate and rank the discovered mathematical concepts based on their complexity and explanatory power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Importing Required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the src directory to the Python path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..', 'src')))\n",
    "\n",
    "from probabilistic_model import mathematical_concept_model\n",
    "from utils import evaluate_concept, concept_complexity, rank_concepts\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"Imports complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Generating and Evaluating Concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Generate concepts\n",
    "input_data = torch.randn(100)\n",
    "concepts, observations = mathematical_concept_model(input_data)\n",
    "\n",
    "# Flatten the list of concepts\n",
    "flat_concepts = [c for level in concepts for c in level]\n",
    "\n",
    "# Evaluate concepts\n",
    "evaluated_concepts = [(c, evaluate_concept(c, input_data)) for c in flat_concepts]\n",
    "\n",
    "print(f\"Generated and evaluated {len(evaluated_concepts)} concepts.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Ranking Concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Rank concepts\n",
    "ranked_concepts = rank_concepts(flat_concepts, input_data)\n",
    "\n",
    "print(\"Top 5 Concepts:\")\n",
    "for i, (concept, score) in enumerate(ranked_concepts[:5], 1):\n",
    "    print(f\"{i}. Concept: {concept}, Score: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4 Visualizing Concept Complexity vs. Explanatory Power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def plot_complexity_vs_explanatory_power(concepts, scores):\n",
    "    complexities = [concept_complexity(c) for c in concepts]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(x=complexities, y=scores)\n",
    "    plt.xlabel('Concept Complexity')\n",
    "    plt.ylabel('Explanatory Power (lower is better)')\n",
    "    plt.title('Concept Complexity vs. Explanatory Power')\n",
    "    \n",
    "    # Annotate some interesting points\n",
    "    for i in range(min(5, len(concepts))):\n",
    "        plt.annotate(f'C{i}', (complexities[i], scores[i]), xytext=(5, 5), \n",
    "                     textcoords='offset points')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "concepts, scores = zip(*ranked_concepts)\n",
    "plot_complexity_vs_explanatory_power(concepts, scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5 Analyzing Concept Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def analyze_concept_distribution(scores):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(scores, kde=True)\n",
    "    plt.xlabel('Explanatory Power Score')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Concept Scores')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Mean score: {np.mean(scores):.4f}\")\n",
    "    print(f\"Median score: {np.median(scores):.4f}\")\n",
    "    print(f\"Standard deviation: {np.std(scores):.4f}\")\n",
    "\n",
    "analyze_concept_distribution(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.6 Identifying Promising Concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def identify_promising_concepts(concepts, scores, threshold_percentile=10):\n",
    "    threshold = np.percentile(scores, threshold_percentile)\n",
    "    promising_concepts = [(c, s) for c, s in zip(concepts, scores) if s <= threshold]\n",
    "    \n",
    "    print(f\"Promising concepts (top {threshold_percentile}%):\")\n",
    "    for i, (concept, score) in enumerate(promising_concepts, 1):\n",
    "        print(f\"{i}. Concept: {concept}, Score: {score:.4f}\")\n",
    "    \n",
    "    return promising_concepts\n",
    "\n",
    "promising_concepts = identify_promising_concepts(concepts, scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.7 Concept Improvement Suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def suggest_improvements(concept, score):\n",
    "    complexity = concept_complexity(concept)\n",
    "    \n",
    "    if complexity > 10 and score > np.median(scores):\n",
    "        return \"Consider simplifying this concept to improve its explanatory power.\"\n",
    "    elif complexity < 5 and score > np.median(scores):\n",
    "        return \"This concept might be too simple. Consider combining it with other concepts.\"\n",
    "    elif score <= np.percentile(scores, 10):\n",
    "        return \"This concept shows promise. Consider refining it further or exploring similar concepts.\"\n",
    "    else:\n",
    "        return \"This concept performs adequately. No specific improvements suggested.\"\n",
    "\n",
    "print(\"Improvement suggestions for top 5 concepts:\")\n",
    "for concept, score in ranked_concepts[:5]:\n",
    "    print(f\"Concept: {concept}\")\n",
    "    print(f\"Suggestion: {suggest_improvements(concept, score)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to evaluate and rank the mathematical concepts discovered by our system. I'm showing how to:\n",
    "\n",
    "1. Generate and evaluate concepts based on their explanatory power\n",
    "2. Rank concepts to identify the most promising ones\n",
    "3. Visualize the trade-off between concept complexity and explanatory power\n",
    "4. Analyze the distribution of concept scores\n",
    "5. Identify particularly promising concepts\n",
    "6. Suggest potential improvements for concepts\n",
    "\n",
    "This evaluation process is crucial for guiding further exploration and refinement in the mathematical invention system. By quantifying the quality of discovered concepts, we can focus our efforts on the most promising areas and iteratively improve our results.\n",
    "\n",
    "The trade-off between complexity and explanatory power is a key consideration in mathematical discovery. Simple concepts that explain a lot of data are particularly valuable, as they often represent fundamental principles. However, more complex concepts might be necessary to capture subtle mathematical relationships.\n",
    "\n",
    "By using this evaluation framework, we can systematically assess the quality of our discovered concepts and guide the system towards more meaningful mathematical inventions."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
